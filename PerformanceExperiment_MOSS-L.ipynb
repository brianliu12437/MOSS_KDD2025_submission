{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3747b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from gurobipy import quicksum\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn\n",
    "from numba import jit\n",
    "import itertools\n",
    "import scipy\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import random    \n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from scipy.sparse import csc_matrix\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "def load_openml(data_id,ordinal = True, y_label = ''):\n",
    "    \"\"\"Load dataset by id from OpenML. If ordinal == True, encode categorical columns \n",
    "    via ordinal encoding. If ordinal == False then encode categorical columns with dummy vars.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset1 = sklearn.datasets.fetch_openml(data_id = data_id,as_frame = True)\n",
    "    name = dataset1.details['name']\n",
    "    X, y = dataset1.data, dataset1.target \n",
    "    data = pd.DataFrame(X,columns = dataset1.feature_names)\n",
    "\n",
    "    if len(y_label) == 0:\n",
    "        data['y'] = y\n",
    "    else:\n",
    "        data['y'] = y[y_label]\n",
    "\n",
    "    #shuffle index\n",
    "    data = data.sample(frac = 1)\n",
    "    y = data['y']\n",
    "    y = y.astype(float)\n",
    "    X = data.drop('y',axis = 1)\n",
    "\n",
    "    #encode categorical columns\n",
    "    cat = list(set(X.columns) - set(X.select_dtypes(include=np.number).columns.tolist()))\n",
    "    \n",
    "    if ordinal == True:\n",
    "        for col in cat:\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "            X[col] = X[col].fillna(max(X[col]+1))\n",
    "        \n",
    "    elif ordinal == False:\n",
    "        X = pd.get_dummies(X,columns = cat)\n",
    "    \n",
    "\n",
    "    return X,y,name\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "get np2darray representation of each rule, 1st column = (features ), 2nd column (split values)\n",
    "all representations are stacked into a list\n",
    "\"\"\"\n",
    "def get_rule_list(tree_list):\n",
    "    rule_list = []\n",
    "    for tree1 in tree_list:\n",
    "        n_nodes = tree1.tree_.node_count\n",
    "        children_left = tree1.tree_.children_left\n",
    "        children_right = tree1.tree_.children_right\n",
    "        feature = tree1.tree_.feature\n",
    "        threshold = tree1.tree_.threshold\n",
    "        value = tree1.tree_.value\n",
    "        #can also represent via values so adjacent leaf nodes are not the same...\n",
    "        \n",
    "        \n",
    "        branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "        for b in branches:\n",
    "  \n",
    "            rule = np.column_stack((feature[b[:-1]],threshold[b[:-1]]))\n",
    "            rule_set = set()\n",
    "            i1 = 0\n",
    "            for r in rule:\n",
    "                if b[i1+1] == children_left[b[i1]]:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'L')\n",
    "                else:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'R')\n",
    "                i1 = i1 + 1\n",
    "                \n",
    "            rule_list.append(rule_set)\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "    \n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "    \n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "\n",
    "def get_uniques(rule_list):\n",
    "    unique_rules = [rule_list[0]]\n",
    "    inds = [0]\n",
    "    ind = 1\n",
    "    for r in rule_list[1:]:\n",
    "        \n",
    "        counter = 0\n",
    "        for r1 in unique_rules:\n",
    "            counter = counter + int(r != r1)\n",
    "        if counter == len(unique_rules):\n",
    "            unique_rules.append(r)\n",
    "            inds.append(ind)\n",
    "            \n",
    "        ind = ind + 1\n",
    "            \n",
    "    counts = []\n",
    "    for u in unique_rules:\n",
    "        counter = 0\n",
    "        for j in rule_list:\n",
    "            if u == j:\n",
    "                counter = counter + 1\n",
    "        counts.append(counter)\n",
    "    return np.array(unique_rules), np.array(inds),np.array(counts)\n",
    "\n",
    "\n",
    "def get_tree_matrix_sparse(X,tree1):\n",
    "    leaf_all = np.where(tree1.tree_.feature < 0)[0]\n",
    "    leaves_index = tree1.apply(X.values)\n",
    "    leaves = np.unique(leaves_index)\n",
    "    values = np.ndarray.flatten(tree1.tree_.value)\n",
    "    leaves_values = [values[i] for i in leaves_index]\n",
    "    df = pd.DataFrame(np.column_stack((range(0,len(leaves_index)),leaves_index,leaves_values))\n",
    "             ,columns = ['instance','node','value'])\n",
    "    setdiff = list(set(leaf_all) - set(np.unique(leaves_index)))\n",
    "    toadd = pd.DataFrame(np.column_stack((np.zeros(len(setdiff)),setdiff,np.zeros(len(setdiff)))),\n",
    "                        columns = ['instance','node','value'])\n",
    "    df = df.append(toadd)\n",
    "    matrix_temp = pd.pivot_table(df, index = 'instance',columns = 'node',values = 'value').fillna(0)\n",
    "    return csc_matrix(matrix_temp.values), matrix_temp.columns.values\n",
    "\n",
    "def get_rule_matrix_sparse(X,tree_list):\n",
    "    matrix_full, nodes = get_tree_matrix_sparse(X,tree_list[0])\n",
    "    node_list = [nodes]\n",
    "    for tree1 in tree_list[1:]:\n",
    "        matrix_temp, nodes = get_tree_matrix_sparse(X,tree1)\n",
    "        node_list.append(nodes)\n",
    "        matrix_full = scipy.sparse.hstack([matrix_full,matrix_temp])\n",
    "    return matrix_full.tocsc(), node_list # node list gives the indicies of the nodes in the tree structure\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "\n",
    "def quantize_X(xTrain,xTest,nbins = 10):\n",
    "    cols = xTrain.columns\n",
    "    to_replace = cols[[(len(np.unique(xTrain[c]))>=nbins) | (len(np.unique(xTest[c]))>=nbins)  for c in cols]]\n",
    "    #pd.qcut(xTrain.rank(method='first')[col, bins)\n",
    "    for col in to_replace:\n",
    "        xTrain[col] = pd.qcut(xTrain.rank(method='first')[col],nbins,labels=False)\n",
    "        xTest[col]= pd.qcut(xTest.rank(method='first')[col],nbins,labels=False)\n",
    "    return xTrain, xTest\n",
    "        \n",
    "\"\"\"\n",
    "get np2darray representation of each rule, 1st column = (features ), 2nd column (split values)\n",
    "all representations are stacked into a list\n",
    "\"\"\"\n",
    "def get_rule_list(tree_list):\n",
    "    rule_list = []\n",
    "    for tree1 in tree_list:\n",
    "        n_nodes = tree1.tree_.node_count\n",
    "        children_left = tree1.tree_.children_left\n",
    "        children_right = tree1.tree_.children_right\n",
    "        feature = tree1.tree_.feature\n",
    "        threshold = tree1.tree_.threshold\n",
    "        value = tree1.tree_.value\n",
    "        #can also represent via values so adjacent leaf nodes are not the same...\n",
    "        \n",
    "        \n",
    "        branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "        for b in branches:\n",
    "  \n",
    "            rule = np.column_stack((feature[b[:-1]],threshold[b[:-1]]))\n",
    "            rule_set = set()\n",
    "            i1 = 0\n",
    "            for r in rule:\n",
    "                if b[i1+1] == children_left[b[i1]]:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'L')\n",
    "                else:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'R')\n",
    "                i1 = i1 + 1\n",
    "                \n",
    "            rule_list.append(rule_set)\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "    \n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "    \n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "\n",
    "def get_uniques(rule_list):\n",
    "    unique_rules = [rule_list[0]]\n",
    "    inds = [0]\n",
    "    ind = 1\n",
    "    for r in rule_list[1:]:\n",
    "        \n",
    "        counter = 0\n",
    "        for r1 in unique_rules:\n",
    "            counter = counter + int(r != r1)\n",
    "        if counter == len(unique_rules):\n",
    "            unique_rules.append(r)\n",
    "            inds.append(ind)\n",
    "            \n",
    "        ind = ind + 1\n",
    "            \n",
    "    counts = []\n",
    "    for u in unique_rules:\n",
    "        counter = 0\n",
    "        for j in rule_list:\n",
    "            if u == j:\n",
    "                counter = counter + 1\n",
    "        counts.append(counter)\n",
    "    return np.array(unique_rules), np.array(inds),np.array(counts)\n",
    "\n",
    "\n",
    "def get_tree_matrix_sparse(X,tree1):\n",
    "    leaf_all = np.where(tree1.tree_.feature < 0)[0]\n",
    "    leaves_index = tree1.apply(X.values)\n",
    "    leaves = np.unique(leaves_index)\n",
    "    values = np.ndarray.flatten(tree1.tree_.value)\n",
    "    leaves_values = [values[i] for i in leaves_index]\n",
    "    df = pd.DataFrame(np.column_stack((range(0,len(leaves_index)),leaves_index,leaves_values))\n",
    "             ,columns = ['instance','node','value'])\n",
    "    setdiff = list(set(leaf_all) - set(np.unique(leaves_index)))\n",
    "    toadd = pd.DataFrame(np.column_stack((np.zeros(len(setdiff)),setdiff,np.zeros(len(setdiff)))),\n",
    "                        columns = ['instance','node','value'])\n",
    "    df = df.append(toadd)\n",
    "    matrix_temp = pd.pivot_table(df, index = 'instance',columns = 'node',values = 'value').fillna(0)\n",
    "    return csc_matrix(matrix_temp.values), matrix_temp.columns.values\n",
    "\n",
    "def get_rule_matrix_sparse(X,tree_list):\n",
    "    matrix_full, nodes = get_tree_matrix_sparse(X,tree_list[0])\n",
    "    node_list = [nodes]\n",
    "    for tree1 in tree_list[1:]:\n",
    "        matrix_temp, nodes = get_tree_matrix_sparse(X,tree1)\n",
    "        node_list.append(nodes)\n",
    "        matrix_full = scipy.sparse.hstack([matrix_full,matrix_temp])\n",
    "    return matrix_full.tocsc(), node_list # node list gives the indicies of the nodes in the tree structure\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "\n",
    "def quantize_X(xTrain,xTest,nbins = 10):\n",
    "    cols = xTrain.columns\n",
    "    to_replace = cols[[(len(np.unique(xTrain[c]))>=nbins) | (len(np.unique(xTest[c]))>=nbins)  for c in cols]]\n",
    "    #pd.qcut(xTrain.rank(method='first')[col, bins)\n",
    "    for col in to_replace:\n",
    "        xTrain[col] = pd.qcut(xTrain.rank(method='first')[col],nbins,labels=False)\n",
    "        xTest[col]= pd.qcut(xTest.rank(method='first')[col],nbins,labels=False)\n",
    "        \n",
    "    return xTrain, xTest\n",
    "        \n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from numba import jit\n",
    "from gurobipy import quicksum\n",
    "### Entire path functions\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_regression_cost_subgradient(X,y,s,lambda_ridge):\n",
    "    \"\"\"\n",
    "    Must all be numpy arrays\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    X_sub = X[:,s==1]\n",
    "    \n",
    "    alpha_star = y - X_sub@np.linalg.inv(np.diag(np.ones(np.sum(s))/lambda_ridge) + \\\n",
    "                                  np.transpose(X_sub)@X_sub)@np.transpose(X_sub)@y\n",
    "    \n",
    "    c = 0.5*y@alpha_star\n",
    "    \n",
    "    subgradients = np.zeros(p)\n",
    "    for j in range(0,p):\n",
    "        subgradients[j] = -lambda_ridge*0.5 *(X[:,j]@alpha_star)**2\n",
    "    \n",
    "    return c, alpha_star , subgradients\n",
    "\n",
    "\n",
    "def get_stability_path_k(ucounts, nonzero, by = 1):\n",
    "    start = 0\n",
    "    end = nonzero\n",
    "    sorted1 = np.sort(ucounts)\n",
    "\n",
    "    K_range = []\n",
    "    while start < len(sorted1):\n",
    "        K_range.append(np.sum(sorted1[start:end]))\n",
    "        start = start + 1\n",
    "        end = end + 1\n",
    "    K_range = np.flip(np.unique(K_range))\n",
    "    return K_range[::by]\n",
    "\n",
    "def epsilon_const_path(X,y, lambda_ridge, nonzero, ucounts, k_range,\n",
    "                            tol,debias =  False, verbose = False, time_limit = 120, time_limit_gurobi = 60):\n",
    "    regressor = gp.Model()\n",
    "    dim = X.shape[1]\n",
    "    nu = regressor.addVar(name=\"nu\") # Weights\n",
    "    s_new = regressor.addVars(dim, vtype=GRB.BINARY, name=\"s_new\") \n",
    "\n",
    "    regressor.setObjective(nu, GRB.MINIMIZE)\n",
    "    regressor.addConstr(quicksum(s_new) <= nonzero) # Budget constraint\n",
    "    regressor.addConstr(nu>=0)\n",
    "\n",
    "\n",
    "    #### settings\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = time_limit_gurobi\n",
    "    regressor.params.mipgap = 0.001    \n",
    "\n",
    "\n",
    "    #stability_budget = regressor.addVar(name=\"stability_budget\")\n",
    "    #regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= stability_budget)\n",
    "\n",
    "    stab_constraint = regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= 0)\n",
    "    results_w = []\n",
    "    results_obj = []\n",
    "    for k in k_range:\n",
    "\n",
    "        regressor.remove(stab_constraint)\n",
    "        stab_constraint = regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= k)\n",
    "\n",
    "        #regressor.setAttr(\"LB\", stability_budget, k)\n",
    "        #regressor.setAttr(\"UB\", stability_budget, k)\n",
    "\n",
    "        cost = 10**9\n",
    "        nu1 = 0\n",
    "\n",
    "\n",
    "        t1 = time.time()\n",
    "        while  (nu1 - cost)/cost < -tol:\n",
    "            regressor.optimize()\n",
    "            s = np.array([s_new[i].X for i in range(dim)])\n",
    "            nu1 = nu.X\n",
    "            s = s.astype(int)\n",
    "            cost, alpha_star, subgradient = get_regression_cost_subgradient(X,y,s,lambda_ridge)  \n",
    "            regressor.addConstr(nu >= cost + quicksum([ subgradient[i]*(s_new[i] -s[i]) for i in range(dim)]))\n",
    "            if (time.time() - t1) > time_limit:\n",
    "                break\n",
    "            \n",
    "            #print(len(regressor.getConstrs()))\n",
    "\n",
    "        results_obj.append([nu1,cost])\n",
    "        X_sub = X[:,s==1]\n",
    "        if debias == True:\n",
    "            inv = np.linalg.inv(np.transpose(X_sub)@X_sub + np.diag(np.ones(np.sum(s))*0.001))\n",
    "        else:\n",
    "            inv = np.linalg.inv(np.transpose(X_sub)@X_sub + np.diag(np.ones(np.sum(s))/lambda_ridge))\n",
    "\n",
    "        w_sub = inv@np.transpose(X_sub)@y\n",
    "        w = np.zeros(len(s))\n",
    "        w[s==1] = w_sub\n",
    "        results_w.append(w)\n",
    "        print(k,nu1,cost, time.time()-t1)\n",
    "        \n",
    "    return np.array(results_w),results_obj\n",
    "\n",
    "def r_squared(yTest,pred,yTrain):\n",
    "    top = np.sum((yTest - pred)**2)\n",
    "    bottom = np.sum((yTest - np.mean(yTrain))**2)\n",
    "    return 1 - top/bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2248cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heuristic Helper Functions\n",
    "\n",
    "def mcpthresh(w_j,lambda1,gamma):\n",
    "    if np.abs(w_j) <= lambda1*gamma:\n",
    "        return soft_threshold(w_j,lambda1)/(1-(1/gamma))\n",
    "    else:\n",
    "        return w_j\n",
    "    \n",
    "def soft_threshold(w, lambda_):\n",
    "    return (w / abs(w)) * max(abs(w) - lambda_, 0) if abs(w) > lambda_ else 0\n",
    "\n",
    "def mcploss(w,lambda1,gamma):\n",
    "    cond1 = (w <= lambda1*gamma)\n",
    "    cond2 = (w >= lambda1*gamma)\n",
    "    mcp = np.zeros(len(w))\n",
    "    mcp[cond1] = lambda1*w[cond1]- (w[cond1]*w[cond1])/(2*gamma)\n",
    "    mcp[cond2] = 0.5*gamma*lambda1*lambda1\n",
    "    return sum(mcp)\n",
    "\n",
    "def mcpsingle(w_j,lambda1,gamma):\n",
    "    if np.abs(w_j) <= lambda1*gamma:\n",
    "        return lambda1*w_j - (w_j*w_j)/(2*gamma)\n",
    "    else:\n",
    "        return 0.5*gamma*lambda1*lambda1\n",
    "\n",
    "def MCP_CD(y,M,lambda1,gamma,threshold= 10**-3,ws = []):\n",
    "    n = M.shape[0]\n",
    "    p = M.shape[1]\n",
    "\n",
    "    loss_sequence = []\n",
    "    cycle_loss = []\n",
    "    converged = False\n",
    "    \n",
    "    if len(ws) == 0:\n",
    "        w = np.zeros(p)\n",
    "    else:\n",
    "        w = ws\n",
    "    \n",
    "    r = y - M@w\n",
    "    mcp_penalty = mcploss(np.abs(w),lambda1,gamma)\n",
    "    ind = 0\n",
    "    while converged == False:\n",
    "        j = ind%p\n",
    "        M_j = M[:,j]\n",
    "        w_j = w[j]\n",
    "\n",
    "        r = r + M_j*w_j\n",
    "        mcp_penalty = mcp_penalty - mcpsingle(np.abs(w_j),lambda1,gamma)\n",
    "\n",
    "\n",
    "        w_sol = M_j@r/(M_j@M_j)\n",
    "        w[j] = mcpthresh(w_sol,lambda1,gamma)\n",
    "\n",
    "        r = r - M_j*w[j]\n",
    "        mcp_penalty = mcp_penalty + mcpsingle(np.abs(w[j]),lambda1,gamma)\n",
    "\n",
    "        loss = (0.5/n)*r@r + mcp_penalty\n",
    "        loss_sequence.append(loss)\n",
    "\n",
    "        ind = ind + 1\n",
    "\n",
    "        if (j == 0):\n",
    "            cycle_loss.append(loss)\n",
    "            if len(cycle_loss) >= 2:\n",
    "                converged = np.abs(cycle_loss[-1]-cycle_loss[-2])<= threshold\n",
    "    return w, loss_sequence\n",
    "\n",
    "\n",
    "#0.5*||y - Mw||_2^2 -    (1-rho)/rho  * sum(ucounts(w!=0)) + lambda_s/rho (sum(w!=0))  \n",
    "#objective\n",
    "\n",
    "@jit(nopython=True)\n",
    "def JIT_cd(j,r, M_j, w, rho,lambda_s, ucounts, n):\n",
    "    r = r + M_j*w[j]\n",
    "    w_star = M_j@r/(M_j@M_j)\n",
    "    obj1 = 0.5*(r-M_j*w_star)@(r-M_j*w_star) + lambda_s/rho - ucounts[j]*(1-rho)/rho\n",
    "    obj2 = 0.5*r@r\n",
    "        \n",
    "        \n",
    "    if obj2 <= obj1:\n",
    "        w[j] = 0\n",
    "    else:\n",
    "        w[j] = w_star\n",
    "            \n",
    "    r = r - M_j*w[j]\n",
    "    loss = (0.5*rho*r@r - (1-rho)*np.sum(ucounts[w!=0]) + lambda_s*np.sum(w!=0))/n \n",
    "    return w,r,loss\n",
    "\n",
    "def CD_L0(y,M,ucounts, rho,lambda_s, threshold, ws = []):\n",
    "    n = M.shape[0]\n",
    "    p = M.shape[1]\n",
    "    \n",
    "    if len(ws) == 0:\n",
    "        w = np.zeros(p)\n",
    "        r = y\n",
    "    else:\n",
    "        w = ws\n",
    "        r = y - M@w\n",
    "    \n",
    "    ind = 0\n",
    "    loss_sequence = []\n",
    "    cycle_loss = []\n",
    "    converged = False\n",
    "    \n",
    "    \n",
    "    while converged == False:\n",
    "        j = ind % p\n",
    "        M_j = M[:,j]\n",
    "        \n",
    "        w, r,loss = JIT_cd(j,r, M_j, w, rho,lambda_s, ucounts, n)\n",
    "        \n",
    "        loss_sequence.append(loss) \n",
    "        if (j == 0):\n",
    "            cycle_loss.append(loss)\n",
    "            if len(cycle_loss) >= 2:\n",
    "                converged = np.abs(cycle_loss[-1]-cycle_loss[-2])<= threshold\n",
    "        ind = ind + 1\n",
    "    return w, loss_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "bins = 10\n",
    "ntrees = 500\n",
    "non_zero1 = 15\n",
    "verbose = False\n",
    "time_limit = 600\n",
    "time_limit_gurobi = 120\n",
    "debias = True\n",
    "tol = 0.05\n",
    "\n",
    "rho_range = np.arange(0.25,1.0,.025)\n",
    "rho_range = np.append(rho_range,0.99)\n",
    "arange = np.flip(np.logspace(0,2,200))\n",
    "lrange = np.logspace(0,-3,100)\n",
    "gamma = 1.1\n",
    "\n",
    "ids = [\n",
    "195,\n",
    "505,\n",
    "560,\n",
    "690,\n",
    "519,\n",
    "196,\n",
    "1027,\n",
    "547,\n",
    "223,\n",
    "541,\n",
    "41021,\n",
    "315,\n",
    "507,\n",
    "529,\n",
    "183,\n",
    "42570,\n",
    "405,\n",
    "294,\n",
    "287,\n",
    "503,\n",
    "308,\n",
    "558,\n",
    "227,\n",
    "189,\n",
    "296,\n",
    "201,\n",
    "216,\n",
    "537,\n",
    "574,\n",
    "344,\n",
    "215,\n",
    "564,\n",
    "42225]\n",
    "print(len(ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "for id1 in ids:\n",
    "    try:\n",
    "\n",
    "        X1,y1,name = load_openml(id1,ordinal = False, y_label = '')\n",
    "        lambda_ridge = 0.01\n",
    "        if X1.shape[0] <= 1000:\n",
    "            lambda_ridge = 0.05\n",
    "\n",
    "        print(name,X1.shape,lambda_ridge)\n",
    "        fire_rules = []\n",
    "        fire_perfs = []\n",
    "        sirus_rules = []\n",
    "        sirus_perfs = []\n",
    "        rulefit_rules = []\n",
    "        rulefit_perfs = []\n",
    "\n",
    "        multiobj_rules = []\n",
    "        multiobj_perfs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        kf = KFold(n_splits=10)\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X1)):\n",
    "\n",
    "            xTrain = X1.iloc[train_index]\n",
    "            yTrain = y1.iloc[train_index]\n",
    "            xTest = X1.iloc[test_index]\n",
    "            yTest = y1.iloc[test_index]\n",
    "            xTrain = xTrain.fillna(xTrain.median())\n",
    "            xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "\n",
    "\n",
    "            xTrain,xTest = quantize_X(xTrain,xTest,nbins = bins)\n",
    "            yscaler = preprocessing.StandardScaler()\n",
    "            yTrain = yscaler.fit_transform(yTrain.values.reshape(-1, 1))\n",
    "            yTest = yscaler.transform(yTest.values.reshape(-1, 1))\n",
    "            yTrain = pd.Series(yTrain.flatten())\n",
    "            yTrain.index = xTrain.index\n",
    "            yTest = pd.Series(yTest.flatten())\n",
    "            yTest.index = xTest.index\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            rf = RandomForestRegressor(n_estimators = ntrees, n_jobs = -1,\n",
    "                                   max_features = .3333,max_depth = 2).fit(xTrain,yTrain)\n",
    "\n",
    "            A, _ = get_rule_matrix_sparse(xTrain,rf.estimators_)\n",
    "            A_test, _ = get_rule_matrix_sparse(xTest,rf.estimators_)\n",
    "            A = A.toarray()\n",
    "            A_test = A_test.toarray()\n",
    "\n",
    "\n",
    "\n",
    "            tree_list = rf.estimators_\n",
    "            rule_list = get_rule_list(tree_list)\n",
    "            unique_rules, uindex,  ucounts = get_uniques(rule_list)\n",
    "            p_unique = len(unique_rules)\n",
    "\n",
    "            inds_top = np.argsort(-ucounts)[:non_zero1]\n",
    "\n",
    "            if (np.sort(ucounts)[::-1][non_zero1-1] == np.sort(ucounts)[::-1][non_zero1]):\n",
    "                inds_top = inds_top[ucounts[inds_top] != np.sort(ucounts)[::-1][non_zero1]]\n",
    "\n",
    "            non_zero = len(inds_top)\n",
    "            sirus = list(np.array(unique_rules)[inds_top])\n",
    "            sirus_rules.append(sirus)\n",
    "\n",
    "            A_u = A[:,uindex]\n",
    "            A_test_u = A_test[:,uindex]\n",
    "\n",
    "            A1 = A_u[:,inds_top]\n",
    "            A_test1 = A_test_u[:,inds_top]\n",
    "            ridge = 0.001\n",
    "            alpha = np.dot((np.dot(np.linalg.inv(np.dot(A1.T,A1) + np.diag(np.repeat(ridge,len(A1[0])))),A1.T)),yTrain)\n",
    "            sirus_perfs.append(r_squared(yTest,A_test1@alpha,yTrain))\n",
    "\n",
    "            s1 = preprocessing.StandardScaler()\n",
    "            M = s1.fit_transform(A_u)\n",
    "            M_test = s1.transform(A_test_u)\n",
    "\n",
    "            del A\n",
    "            del A_test\n",
    "            del A1\n",
    "            del A_test1\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            ### run multi_obj_heuristic \n",
    "\n",
    "            w_all = []\n",
    "            for rho in rho_range:\n",
    "                print('rho:', rho)\n",
    "                ws = []\n",
    "                coef_heuristic = []\n",
    "                for alpha_lasso in arange:\n",
    "                    w, l2 = CD_L0(yTrain.values,M,ucounts, rho,alpha_lasso, 10**-6, ws = ws)\n",
    "                    coef_heuristic.append(w)\n",
    "                    ws = copy.deepcopy(w)\n",
    "\n",
    "                heuristic_best = np.array(coef_heuristic)[[sum(w!=0)<=non_zero for w in coef_heuristic]][-1]\n",
    "                w_all.append(heuristic_best)\n",
    "\n",
    "\n",
    "            mobj_temp = []\n",
    "            mobj_temp_rules = []\n",
    "            for w in w_all:\n",
    "                mobj_temp.append(r_squared(yTest,M_test@w,yTrain))\n",
    "                mobj_temp_rules.append(list(np.array(unique_rules)[w!=0]))\n",
    "\n",
    "            multiobj_perfs.append(mobj_temp)\n",
    "            multiobj_rules.append(mobj_temp_rules)\n",
    "            \n",
    "            coef_MCP = []\n",
    "            ws_MCP = []\n",
    "            for lambda1 in lrange:\n",
    "                w_MCP, ls = MCP_CD(yTrain.values,M,lambda1,gamma,10**-3,ws = ws_MCP)\n",
    "                coef_MCP.append(w_MCP)\n",
    "                ws_MCP = w_MCP.copy()\n",
    "\n",
    "            fire_best = np.array(coef_MCP)[[sum(w!=0)<=non_zero for w in coef_MCP]][-1]\n",
    "            fire_error = r_squared(yTest,M_test@fire_best,yTrain)\n",
    "            fire_rules.append(list(np.array(unique_rules)[fire_best!=0]))\n",
    "            fire_perfs.append(fire_error)\n",
    "\n",
    "            print(i)\n",
    "\n",
    "        res_dict = {}\n",
    "        res_dict['sirus_rules'] =  sirus_rules\n",
    "        res_dict['sirus_perfs'] =  sirus_perfs\n",
    "        res_dict['multiobj_heuristic_rules'] =  multiobj_rules\n",
    "        res_dict['multiobj_heuristic_perfs'] =  multiobj_perfs\n",
    "        name1 = name +'_' + str(id1) +'_trees_' + str(ntrees) + '_ridge_' + \\\n",
    "                str(lambda_ridge) + '_tol_' + str(tol) +'_debias_' + str(debias) +'.pickle'\n",
    "\n",
    "        name1 = 'results_heuristics_granular/pickles/' + name1\n",
    "\n",
    "        with open(name1, 'wb') as handle:\n",
    "            pickle.dump(res_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "        multiobj_perfs = np.array(multiobj_perfs)\n",
    "        multiobj_perf_means = multiobj_perfs.mean(axis = 0)\n",
    "\n",
    "        multiobj_stability = []\n",
    "        for i in range(0,len(multiobj_rules[0])):\n",
    "            temp = []\n",
    "            for i1 in range(0,len(multiobj_rules)):\n",
    "                for i2 in range(i1+1,len(multiobj_rules)):\n",
    "                    rules1 = list(get_uniques(multiobj_rules[i1][i])[0])   \n",
    "                    rules2 = list(get_uniques(multiobj_rules[i2][i])[0])\n",
    "                    temp.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "            multiobj_stability.append(temp)\n",
    "        multiobj_stability = np.array(multiobj_stability)\n",
    "        multiobj_stability_means = multiobj_stability.mean(axis = 1)\n",
    "        \n",
    "        fire_stability = []\n",
    "        for i1 in range(0,len(fire_rules)):\n",
    "            for i2 in range(i1+1,len(fire_rules)):\n",
    "                rules1 = list(get_uniques(fire_rules[i1])[0])   \n",
    "                rules2 = list(get_uniques(fire_rules[i2])[0])\n",
    "                fire_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "\n",
    "        sirus_stability = []\n",
    "        for i1 in range(0,len(sirus_rules)):\n",
    "            for i2 in range(i1+1,len(sirus_rules)):\n",
    "                rules1 = list(get_uniques(sirus_rules[i1])[0])   \n",
    "                rules2 = list(get_uniques(sirus_rules[i2])[0])\n",
    "                sirus_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "        print('fire', np.mean(fire_stability), 1-np.mean(fire_perfs))    \n",
    "        \n",
    "        plt.scatter(1-np.mean(fire_perfs), np.mean(fire_stability), label = 'fire', color = 'green')\n",
    "        plt.scatter(1-multiobj_perf_means[1:],multiobj_stability_means[1:],label = 'multiobj', alpha = .3)\n",
    "        plt.scatter(1-multiobj_perf_means[0],multiobj_stability_means[0],label = 'multiobj_stability', color = 'C0',alpha = .4)\n",
    "\n",
    "        temp_list = list(range(1,len(multiobj_perf_means[1:]) + 1))\n",
    "        for i in range(len(multiobj_perf_means[1:],)):\n",
    "            plt.annotate(temp_list[i], (1-multiobj_perf_means[1:][i], multiobj_stability_means[1:][i]))\n",
    "\n",
    "\n",
    "        plt.scatter(1-np.mean(sirus_perfs),np.mean(sirus_stability),label = 'sirus', color = 'red')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.xlabel('1 - R^2')\n",
    "        plt.ylabel('Stability')\n",
    "        name2 = 'results_heuristics_granular/plots/' + name +'_' + str(id1) +'_trees_' + str(ntrees) + '_ridge_' + \\\n",
    "                str(lambda_ridge) + '_tol_' + str(tol) +'_debias_' + str(debias) +'.pdf'\n",
    "        plt.title(name2)\n",
    "        plt.savefig(name2)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(name)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
