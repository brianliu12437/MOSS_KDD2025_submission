{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27866025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from gurobipy import quicksum\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn\n",
    "from numba import jit\n",
    "import itertools\n",
    "import scipy\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import random    \n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from scipy.sparse import csc_matrix\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "matplotlib.rcParams.update({})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#### Helper Functions\n",
    "\n",
    "import sys\n",
    "def load_openml(data_id,ordinal = True, y_label = ''):\n",
    "    \"\"\"Load dataset by id from OpenML. If ordinal == True, encode categorical columns \n",
    "    via ordinal encoding. If ordinal == False then encode categorical columns with dummy vars.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset1 = sklearn.datasets.fetch_openml(data_id = data_id,as_frame = True)\n",
    "    name = dataset1.details['name']\n",
    "    X, y = dataset1.data, dataset1.target \n",
    "    data = pd.DataFrame(X,columns = dataset1.feature_names)\n",
    "\n",
    "    if len(y_label) == 0:\n",
    "        data['y'] = y\n",
    "    else:\n",
    "        data['y'] = y[y_label]\n",
    "\n",
    "    #shuffle index\n",
    "    data = data.sample(frac = 1)\n",
    "    y = data['y']\n",
    "    y = y.astype(float)\n",
    "    X = data.drop('y',axis = 1)\n",
    "\n",
    "    #encode categorical columns\n",
    "    cat = list(set(X.columns) - set(X.select_dtypes(include=np.number).columns.tolist()))\n",
    "    \n",
    "    if ordinal == True:\n",
    "        for col in cat:\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "            X[col] = X[col].fillna(max(X[col]+1))\n",
    "        \n",
    "    elif ordinal == False:\n",
    "        X = pd.get_dummies(X,columns = cat)\n",
    "    \n",
    "\n",
    "    return X,y,name\n",
    "\n",
    "\n",
    "def quantize_X(xTrain,xTest,nbins = 10):\n",
    "    cols = xTrain.columns\n",
    "    to_replace = cols[[(len(np.unique(xTrain[c]))>=nbins) for c in cols]]\n",
    "    #pd.qcut(xTrain.rank(method='first')[col, bins)\n",
    "    for col in to_replace:\n",
    "        xTrain[col],bins = pd.qcut(xTrain[col],nbins,labels=False,retbins=True,duplicates = 'drop')\n",
    "        bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "        xTest[col]= pd.cut(xTest[col],bins,labels=False)\n",
    "        \n",
    "    return xTrain, xTest\n",
    "\n",
    "\"\"\"\n",
    "get np2darray representation of each rule, 1st column = (features ), 2nd column (split values)\n",
    "all representations are stacked into a list\n",
    "\"\"\"\n",
    "def get_rule_list(tree_list):\n",
    "    rule_list = []\n",
    "    for tree1 in tree_list:\n",
    "        n_nodes = tree1.tree_.node_count\n",
    "        children_left = tree1.tree_.children_left\n",
    "        children_right = tree1.tree_.children_right\n",
    "        feature = tree1.tree_.feature\n",
    "        threshold = tree1.tree_.threshold\n",
    "        value = tree1.tree_.value\n",
    "        #can also represent via values so adjacent leaf nodes are not the same...\n",
    "        \n",
    "        \n",
    "        branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "        for b in branches:\n",
    "  \n",
    "            rule = np.column_stack((feature[b[:-1]],threshold[b[:-1]]))\n",
    "            rule_set = set()\n",
    "            i1 = 0\n",
    "            for r in rule:\n",
    "                if b[i1+1] == children_left[b[i1]]:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'L')\n",
    "                else:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'R')\n",
    "                i1 = i1 + 1\n",
    "                \n",
    "            rule_list.append(rule_set)\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "    \n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "    \n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "\n",
    "def get_uniques(rule_list):\n",
    "    unique_rules = [rule_list[0]]\n",
    "    inds = [0]\n",
    "    ind = 1\n",
    "    for r in rule_list[1:]:\n",
    "        \n",
    "        counter = 0\n",
    "        for r1 in unique_rules:\n",
    "            counter = counter + int(r != r1)\n",
    "        if counter == len(unique_rules):\n",
    "            unique_rules.append(r)\n",
    "            inds.append(ind)\n",
    "            \n",
    "        ind = ind + 1\n",
    "            \n",
    "    counts = []\n",
    "    for u in unique_rules:\n",
    "        counter = 0\n",
    "        for j in rule_list:\n",
    "            if u == j:\n",
    "                counter = counter + 1\n",
    "        counts.append(counter)\n",
    "    return np.array(unique_rules), np.array(inds),np.array(counts)\n",
    "\n",
    "\n",
    "def get_tree_matrix_sparse(X,tree1):\n",
    "    leaf_all = np.where(tree1.tree_.feature < 0)[0]\n",
    "    leaves_index = tree1.apply(X.values)\n",
    "    leaves = np.unique(leaves_index)\n",
    "    values = np.ndarray.flatten(tree1.tree_.value)\n",
    "    leaves_values = [values[i] for i in leaves_index]\n",
    "    df = pd.DataFrame(np.column_stack((range(0,len(leaves_index)),leaves_index,leaves_values))\n",
    "             ,columns = ['instance','node','value'])\n",
    "    setdiff = list(set(leaf_all) - set(np.unique(leaves_index)))\n",
    "    toadd = pd.DataFrame(np.column_stack((np.zeros(len(setdiff)),setdiff,np.zeros(len(setdiff)))),\n",
    "                        columns = ['instance','node','value'])\n",
    "    df = df.append(toadd)\n",
    "    matrix_temp = pd.pivot_table(df, index = 'instance',columns = 'node',values = 'value').fillna(0)\n",
    "    return csc_matrix(matrix_temp.values), matrix_temp.columns.values\n",
    "\n",
    "def get_rule_matrix_sparse(X,tree_list):\n",
    "    matrix_full, nodes = get_tree_matrix_sparse(X,tree_list[0])\n",
    "    node_list = [nodes]\n",
    "    for tree1 in tree_list[1:]:\n",
    "        matrix_temp, nodes = get_tree_matrix_sparse(X,tree1)\n",
    "        node_list.append(nodes)\n",
    "        matrix_full = scipy.sparse.hstack([matrix_full,matrix_temp])\n",
    "    return matrix_full.tocsc(), node_list # node list gives the indicies of the nodes in the tree structure\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def r_squared(yTest,pred,yTrain):\n",
    "    top = np.sum((yTest - pred)**2)\n",
    "    bottom = np.sum((yTest - np.mean(yTrain))**2)\n",
    "    return 1 - top/bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84baf3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MOSS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a032ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get np2darray representation of each rule, 1st column = (features ), 2nd column (split values)\n",
    "all representations are stacked into a list\n",
    "\"\"\"\n",
    "def get_rule_list(tree_list):\n",
    "    rule_list = []\n",
    "    for tree1 in tree_list:\n",
    "        n_nodes = tree1.tree_.node_count\n",
    "        children_left = tree1.tree_.children_left\n",
    "        children_right = tree1.tree_.children_right\n",
    "        feature = tree1.tree_.feature\n",
    "        threshold = tree1.tree_.threshold\n",
    "        value = tree1.tree_.value\n",
    "        #can also represent via values so adjacent leaf nodes are not the same...\n",
    "        \n",
    "        \n",
    "        branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "        for b in branches:\n",
    "  \n",
    "            rule = np.column_stack((feature[b[:-1]],threshold[b[:-1]]))\n",
    "            rule_set = set()\n",
    "            i1 = 0\n",
    "            for r in rule:\n",
    "                if b[i1+1] == children_left[b[i1]]:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'L')\n",
    "                else:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'R')\n",
    "                i1 = i1 + 1\n",
    "                \n",
    "            rule_list.append(rule_set)\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "    \n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "    \n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "\n",
    "def get_uniques(rule_list):\n",
    "    unique_rules = [rule_list[0]]\n",
    "    inds = [0]\n",
    "    ind = 1\n",
    "    for r in rule_list[1:]:\n",
    "        \n",
    "        counter = 0\n",
    "        for r1 in unique_rules:\n",
    "            counter = counter + int(r != r1)\n",
    "        if counter == len(unique_rules):\n",
    "            unique_rules.append(r)\n",
    "            inds.append(ind)\n",
    "            \n",
    "        ind = ind + 1\n",
    "            \n",
    "    counts = []\n",
    "    for u in unique_rules:\n",
    "        counter = 0\n",
    "        for j in rule_list:\n",
    "            if u == j:\n",
    "                counter = counter + 1\n",
    "        counts.append(counter)\n",
    "    return np.array(unique_rules), np.array(inds),np.array(counts)\n",
    "\n",
    "\n",
    "def get_tree_matrix_sparse(X,tree1):\n",
    "    leaf_all = np.where(tree1.tree_.feature < 0)[0]\n",
    "    leaves_index = tree1.apply(X.values)\n",
    "    leaves = np.unique(leaves_index)\n",
    "    values = np.ndarray.flatten(tree1.tree_.value)\n",
    "    leaves_values = [values[i] for i in leaves_index]\n",
    "    df = pd.DataFrame(np.column_stack((range(0,len(leaves_index)),leaves_index,leaves_values))\n",
    "             ,columns = ['instance','node','value'])\n",
    "    setdiff = list(set(leaf_all) - set(np.unique(leaves_index)))\n",
    "    toadd = pd.DataFrame(np.column_stack((np.zeros(len(setdiff)),setdiff,np.zeros(len(setdiff)))),\n",
    "                        columns = ['instance','node','value'])\n",
    "    df = df.append(toadd)\n",
    "    matrix_temp = pd.pivot_table(df, index = 'instance',columns = 'node',values = 'value').fillna(0)\n",
    "    return csc_matrix(matrix_temp.values), matrix_temp.columns.values\n",
    "\n",
    "def get_rule_matrix_sparse(X,tree_list):\n",
    "    matrix_full, nodes = get_tree_matrix_sparse(X,tree_list[0])\n",
    "    node_list = [nodes]\n",
    "    for tree1 in tree_list[1:]:\n",
    "        matrix_temp, nodes = get_tree_matrix_sparse(X,tree1)\n",
    "        node_list.append(nodes)\n",
    "        matrix_full = scipy.sparse.hstack([matrix_full,matrix_temp])\n",
    "    return matrix_full.tocsc(), node_list # node list gives the indicies of the nodes in the tree structure\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "\n",
    "def quantize_X(xTrain,xTest,nbins = 10):\n",
    "    cols = xTrain.columns\n",
    "    to_replace = cols[[(len(np.unique(xTrain[c]))>=nbins) | (len(np.unique(xTest[c]))>=nbins)  for c in cols]]\n",
    "    #pd.qcut(xTrain.rank(method='first')[col, bins)\n",
    "    for col in to_replace:\n",
    "        xTrain[col] = pd.qcut(xTrain.rank(method='first')[col],nbins,labels=False)\n",
    "        xTest[col]= pd.qcut(xTest.rank(method='first')[col],nbins,labels=False)\n",
    "        \n",
    "    return xTrain, xTest\n",
    "        \n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from numba import jit\n",
    "from gurobipy import quicksum\n",
    "### Entire path functions\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_regression_cost_subgradient(X,y,s,lambda_ridge):\n",
    "    \"\"\"\n",
    "    Must all be numpy arrays\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    X_sub = X[:,s==1]\n",
    "    \n",
    "    alpha_star = y - X_sub@np.linalg.inv(np.diag(np.ones(np.sum(s))/lambda_ridge) + \\\n",
    "                                  np.transpose(X_sub)@X_sub)@np.transpose(X_sub)@y\n",
    "    \n",
    "    c = 0.5*y@alpha_star\n",
    "    \n",
    "    subgradients = np.zeros(p)\n",
    "    for j in range(0,p):\n",
    "        subgradients[j] = -lambda_ridge*0.5 *(X[:,j]@alpha_star)**2\n",
    "    \n",
    "    return c, alpha_star , subgradients\n",
    "\n",
    "\n",
    "def get_stability_path_k(ucounts, nonzero, by = 1):\n",
    "    start = 0\n",
    "    end = nonzero\n",
    "    sorted1 = np.sort(ucounts)\n",
    "\n",
    "    K_range = []\n",
    "    while start < len(sorted1):\n",
    "        K_range.append(np.sum(sorted1[start:end]))\n",
    "        start = start + 1\n",
    "        end = end + 1\n",
    "    K_range = np.flip(np.unique(K_range))\n",
    "    return K_range[::by]\n",
    "\n",
    "def epsilon_const_path(X,y, lambda_ridge, nonzero, ucounts, k_range,\n",
    "                            tol,debias =  False, verbose = False, time_limit = 120, time_limit_gurobi = 60):\n",
    "    regressor = gp.Model()\n",
    "    dim = X.shape[1]\n",
    "    nu = regressor.addVar(name=\"nu\") # Weights\n",
    "    s_new = regressor.addVars(dim, vtype=GRB.BINARY, name=\"s_new\") \n",
    "\n",
    "    regressor.setObjective(nu, GRB.MINIMIZE)\n",
    "    regressor.addConstr(quicksum(s_new) <= nonzero) # Budget constraint\n",
    "    regressor.addConstr(nu>=0)\n",
    "\n",
    "\n",
    "    #### settings\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = time_limit_gurobi\n",
    "    regressor.params.mipgap = 0.001    \n",
    "\n",
    "\n",
    "    #stability_budget = regressor.addVar(name=\"stability_budget\")\n",
    "    #regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= stability_budget)\n",
    "\n",
    "    stab_constraint = regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= 0)\n",
    "    results_w = []\n",
    "    results_obj = []\n",
    "    for k in k_range:\n",
    "\n",
    "        regressor.remove(stab_constraint)\n",
    "        stab_constraint = regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= k)\n",
    "\n",
    "        #regressor.setAttr(\"LB\", stability_budget, k)\n",
    "        #regressor.setAttr(\"UB\", stability_budget, k)\n",
    "\n",
    "        cost = 10**9\n",
    "        nu1 = 0\n",
    "\n",
    "\n",
    "        t1 = time.time()\n",
    "        while  (nu1 - cost)/cost < -tol:\n",
    "            regressor.optimize()\n",
    "            s = np.array([s_new[i].X for i in range(dim)])\n",
    "            nu1 = nu.X\n",
    "            s = s.astype(int)\n",
    "            cost, alpha_star, subgradient = get_regression_cost_subgradient(X,y,s,lambda_ridge)  \n",
    "            regressor.addConstr(nu >= cost + quicksum([ subgradient[i]*(s_new[i] -s[i]) for i in range(dim)]))\n",
    "            if (time.time() - t1) > time_limit:\n",
    "                break\n",
    "            \n",
    "            #print(len(regressor.getConstrs()))\n",
    "\n",
    "        results_obj.append([nu1,cost])\n",
    "        X_sub = X[:,s==1]\n",
    "        if debias == True:\n",
    "            inv = np.linalg.inv(np.transpose(X_sub)@X_sub + np.diag(np.ones(np.sum(s))*0.001))\n",
    "        else:\n",
    "            inv = np.linalg.inv(np.transpose(X_sub)@X_sub + np.diag(np.ones(np.sum(s))/lambda_ridge))\n",
    "\n",
    "        w_sub = inv@np.transpose(X_sub)@y\n",
    "        w = np.zeros(len(s))\n",
    "        w[s==1] = w_sub\n",
    "        results_w.append(w)\n",
    "        print(k,nu1,cost, time.time()-t1)\n",
    "        \n",
    "    return np.array(results_w),results_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93832e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "\n",
    "### experiment parameters\n",
    "\n",
    "bins = 10\n",
    "ntrees = 500\n",
    "non_zero1 = 15\n",
    "verbose = False\n",
    "time_limit = 600\n",
    "time_limit_gurobi = 120\n",
    "debias = True\n",
    "tol = 0.05\n",
    "\n",
    "# all datasets from openML, loaded using the load_openml function\n",
    "ids = [\n",
    "195,\n",
    "505,\n",
    "560,\n",
    "690,\n",
    "519,\n",
    "196,\n",
    "1027,\n",
    "547,\n",
    "223,\n",
    "541,\n",
    "41021,\n",
    "315,\n",
    "507,\n",
    "529,\n",
    "183,\n",
    "42570,\n",
    "405,\n",
    "294,\n",
    "287,\n",
    "503,\n",
    "308,\n",
    "558,\n",
    "227,\n",
    "189,\n",
    "296,\n",
    "201,\n",
    "216,\n",
    "537,\n",
    "574,\n",
    "344,\n",
    "215,\n",
    "564,\n",
    "42225]\n",
    "print(len(ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9786f",
   "metadata": {},
   "source": [
    "### MOSS-H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "for id1 in ids:\n",
    "    X1,y1,name = load_openml(id1,ordinal = False, y_label = '')\n",
    "    lambda_ridge = 0.01\n",
    "    if X1.shape[0] <= 1000:\n",
    "        lambda_ridge = 0.05\n",
    "\n",
    "    print(name,X1.shape,lambda_ridge)\n",
    "    sirus_rules = []\n",
    "    sirus_perfs = []\n",
    "    rulefit_rules = []\n",
    "    rulefit_perfs = []\n",
    "\n",
    "    multiobj_rules = []\n",
    "    multiobj_perfs = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    kf = KFold(n_splits=10)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X1)):\n",
    "        xTrain = X1.iloc[train_index]\n",
    "        yTrain = y1.iloc[train_index]\n",
    "        xTest = X1.iloc[test_index]\n",
    "        yTest = y1.iloc[test_index]\n",
    "        xTrain = xTrain.fillna(xTrain.median())\n",
    "        xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "        xTrain,xTest = quantize_X(xTrain,xTest,nbins = 10)\n",
    "        yscaler = preprocessing.StandardScaler()\n",
    "        yTrain = yscaler.fit_transform(yTrain.values.reshape(-1, 1))\n",
    "        yTest = yscaler.transform(yTest.values.reshape(-1, 1))\n",
    "        yTrain = pd.Series(yTrain.flatten())\n",
    "        yTrain.index = xTrain.index\n",
    "        yTest = pd.Series(yTest.flatten())\n",
    "        yTest.index = xTest.index\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        rf = RandomForestRegressor(n_estimators = ntrees, n_jobs = -1,\n",
    "                               max_features = .3333,max_depth = 2).fit(xTrain,yTrain)\n",
    "\n",
    "        A, _ = get_rule_matrix_sparse(xTrain,rf.estimators_)\n",
    "        A_test, _ = get_rule_matrix_sparse(xTest,rf.estimators_)\n",
    "        A = A.toarray()\n",
    "        A_test = A_test.toarray()\n",
    "        \n",
    "        \n",
    "        \n",
    "        tree_list = rf.estimators_\n",
    "        rule_list = get_rule_list(tree_list)\n",
    "        unique_rules, uindex,  ucounts = get_uniques(rule_list)\n",
    "        p_unique = len(unique_rules)\n",
    "\n",
    "        inds_top = np.argsort(-ucounts)[:non_zero1]\n",
    "\n",
    "        if (np.sort(ucounts)[::-1][non_zero1-1] == np.sort(ucounts)[::-1][non_zero1]):\n",
    "            inds_top = inds_top[ucounts[inds_top] != np.sort(ucounts)[::-1][non_zero1]]\n",
    "\n",
    "        non_zero = len(inds_top)\n",
    "\n",
    "        sirus = list(np.array(unique_rules)[inds_top])\n",
    "        sirus_rules.append(sirus)\n",
    "        \n",
    "        A_u = A[:,uindex]\n",
    "        A_test_u = A_test[:,uindex]\n",
    "        \n",
    "        A1 = A_u[:,inds_top]\n",
    "        A_test1 = A_test_u[:,inds_top]\n",
    "        ridge = 0.001\n",
    "        alpha = np.dot((np.dot(np.linalg.inv(np.dot(A1.T,A1) + np.diag(np.repeat(ridge,len(A1[0])))),A1.T)),yTrain)\n",
    "        sirus_perfs.append(r_squared(yTest,A_test1@alpha,yTrain))\n",
    "\n",
    "\n",
    "        k_range1 = get_stability_path_k(ucounts, non_zero, by = 1)\n",
    "        k_range = k_range1[:4]\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        w_all,_ = epsilon_const_path(A_u,yTrain.values, lambda_ridge, non_zero, ucounts, k_range,\n",
    "                                tol , debias =  debias, verbose = False, time_limit = time_limit, time_limit_gurobi = time_limit_gurobi)\n",
    "\n",
    "\n",
    "        mobj_temp = []\n",
    "        mobj_temp_rules = []\n",
    "        for w in w_all:\n",
    "            mobj_temp.append(r_squared(yTest,A_test_u@w,yTrain))\n",
    "            mobj_temp_rules.append(list(np.array(unique_rules)[w!=0]))\n",
    "\n",
    "        multiobj_perfs.append(mobj_temp)\n",
    "        multiobj_rules.append(mobj_temp_rules)\n",
    "\n",
    "\n",
    "\n",
    "        #RuleFit\n",
    "        mod = sklearn.linear_model.lasso_path(A_u,yTrain,n_alphas = 1000)\n",
    "        coefs = mod[1].T\n",
    "        if len(np.where([sum(c!=0)==non_zero for c in coefs])[0])>0:\n",
    "            w_l = coefs[np.where([sum(c!=0)==non_zero for c in coefs])[0][0]]\n",
    "            rulefit_rules.append(list(np.array(unique_rules)[w_l!=0]))\n",
    "            rulefit_perfs.append(r_squared(yTest,A_test_u@w_l,yTrain))\n",
    "        #####\n",
    "        print('fold',i, non_zero)\n",
    "    \n",
    "    \n",
    "    res_dict = {}\n",
    "    res_dict['sirus_rules'] =  sirus_rules\n",
    "    res_dict['sirus_perfs'] =  sirus_perfs\n",
    "    res_dict['rulefit_rules'] =  rulefit_rules\n",
    "    res_dict['rulefit_perfs'] =  rulefit_perfs\n",
    "    res_dict['multiobj_rules'] =  multiobj_rules\n",
    "    res_dict['multiobj_perfs'] =  multiobj_perfs\n",
    "    name1 = name +'_' + str(id1) +'_trees_' + str(ntrees) + '_ridge_' + \\\n",
    "            str(lambda_ridge) + '_tol_' + str(tol) +'_debias_' + str(debias) +'.pickle'\n",
    "    \n",
    "    \n",
    "    with open(name1, 'wb') as handle:\n",
    "        pickle.dump(res_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    multiobj_perfs = np.array(multiobj_perfs)\n",
    "    multiobj_perf_means = multiobj_perfs.mean(axis = 0)\n",
    "\n",
    "    multiobj_stability = []\n",
    "    for i in range(0,len(multiobj_rules[0])):\n",
    "        temp = []\n",
    "        for i1 in range(0,len(multiobj_rules)):\n",
    "            for i2 in range(i1+1,len(multiobj_rules)):\n",
    "                rules1 = list(get_uniques(multiobj_rules[i1][i])[0])   \n",
    "                rules2 = list(get_uniques(multiobj_rules[i2][i])[0])\n",
    "                temp.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "        multiobj_stability.append(temp)\n",
    "    multiobj_stability = np.array(multiobj_stability)\n",
    "    multiobj_stability_means = multiobj_stability.mean(axis = 1)\n",
    "    rulefit_stability= []\n",
    "    for i1 in range(0,len(rulefit_rules)):\n",
    "        for i2 in range(i1+1,len(rulefit_rules)):\n",
    "            rules1 = list(get_uniques(rulefit_rules[i1])[0])   \n",
    "            rules2 = list(get_uniques(rulefit_rules[i2])[0])\n",
    "            rulefit_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "    sirus_stability = []\n",
    "    for i1 in range(0,len(sirus_rules)):\n",
    "        for i2 in range(i1+1,len(sirus_rules)):\n",
    "            rules1 = list(get_uniques(sirus_rules[i1])[0])   \n",
    "            rules2 = list(get_uniques(sirus_rules[i2])[0])\n",
    "            sirus_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "\n",
    "    plt.scatter(1-multiobj_perf_means[1:],multiobj_stability_means[1:],label = 'multiobj', alpha = .3)\n",
    "    plt.scatter(1-multiobj_perf_means[0],multiobj_stability_means[0],label = 'multiobj_stability', color = 'C0',alpha = .4)\n",
    "\n",
    "    temp_list = list(range(1,len(multiobj_perf_means[1:]) + 1))\n",
    "    for i in range(len(multiobj_perf_means[1:],)):\n",
    "        plt.annotate(temp_list[i], (1-multiobj_perf_means[1:][i], multiobj_stability_means[1:][i]))\n",
    "\n",
    "    plt.scatter(1- np.mean(rulefit_perfs),np.mean(rulefit_stability),label = 'rulefit', color = 'green')\n",
    "    plt.scatter(1-np.mean(sirus_perfs),np.mean(sirus_stability),label = 'sirus', color = 'red')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('1 - R^2')\n",
    "    plt.ylabel('Stability')\n",
    "    name2 = name +'_' + str(id1) +'_trees_' + str(ntrees) + '_ridge_' + \\\n",
    "            str(lambda_ridge) + '_tol_' + str(tol) +'_debias_' + str(debias) +'.pdf'\n",
    "    plt.title(name2)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ab60e",
   "metadata": {},
   "source": [
    "### MOSS-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66342aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eps = 50\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "for id1 in ids:\n",
    "    X1,y1,name = load_openml(id1,ordinal = False, y_label = '')\n",
    "    lambda_ridge = 0.001\n",
    "    if X1.shape[0] <= 1000:\n",
    "        lambda_ridge = 0.01\n",
    "\n",
    "    print(name,X1.shape,lambda_ridge)\n",
    "    \n",
    "    sirus_rules = []\n",
    "    sirus_perfs = []\n",
    "    rulefit_rules = []\n",
    "    rulefit_perfs = []\n",
    "\n",
    "    multiobj_rules = []\n",
    "    multiobj_perfs = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    kf = KFold(n_splits=10)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X1)):\n",
    "        xTrain = X1.iloc[train_index]\n",
    "        yTrain = y1.iloc[train_index]\n",
    "        xTest = X1.iloc[test_index]\n",
    "        yTest = y1.iloc[test_index]\n",
    "        xTrain = xTrain.fillna(xTrain.median())\n",
    "        xTest = xTest.fillna(xTrain.median())\n",
    "        xTrain,xTest = quantize_X(xTrain,xTest,nbins = bins)\n",
    "    \n",
    "        yscaler = preprocessing.StandardScaler()\n",
    "        yTrain = yscaler.fit_transform(yTrain.values.reshape(-1, 1))\n",
    "        yTest = yscaler.transform(yTest.values.reshape(-1, 1))\n",
    "        yTrain = pd.Series(yTrain.flatten())\n",
    "        yTrain.index = xTrain.index\n",
    "        yTest = pd.Series(yTest.flatten())\n",
    "        yTest.index = xTest.index\n",
    "        \n",
    "        \n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        rf = RandomForestRegressor(n_estimators = ntrees, n_jobs = -1,\n",
    "                               max_features = .3333,max_depth = 2).fit(xTrain,yTrain)\n",
    "\n",
    "        A, _ = get_rule_matrix_sparse(xTrain,rf.estimators_)\n",
    "        A_test, _ = get_rule_matrix_sparse(xTest,rf.estimators_)\n",
    "        A = A.toarray()\n",
    "        A_test = A_test.toarray()\n",
    "        \n",
    "        \n",
    "        \n",
    "        tree_list = rf.estimators_\n",
    "        rule_list = get_rule_list(tree_list)\n",
    "        unique_rules, uindex,  ucounts = get_uniques(rule_list)\n",
    "        p_unique = len(unique_rules)\n",
    "\n",
    "        inds_top = np.argsort(-ucounts)[:non_zero1]\n",
    "\n",
    "        if (np.sort(ucounts)[::-1][non_zero1-1] == np.sort(ucounts)[::-1][non_zero1]):\n",
    "            inds_top = inds_top[ucounts[inds_top] != np.sort(ucounts)[::-1][non_zero1]]\n",
    "\n",
    "        non_zero = len(inds_top)\n",
    "\n",
    "        sirus = list(np.array(unique_rules)[inds_top])\n",
    "        sirus_rules.append(sirus)\n",
    "        \n",
    "        A_u = A[:,uindex]\n",
    "        A_test_u = A_test[:,uindex]\n",
    "        \n",
    "        A1 = A_u[:,inds_top]\n",
    "        A_test1 = A_test_u[:,inds_top]\n",
    "        ridge = 0.001\n",
    "        alpha = np.dot((np.dot(np.linalg.inv(np.dot(A1.T,A1) + np.diag(np.repeat(ridge,len(A1[0])))),A1.T)),yTrain)\n",
    "        sirus_perfs.append(r_squared(yTest,A_test1@alpha,yTrain))\n",
    "\n",
    "\n",
    "        k_range1 = get_stability_path_k(ucounts, non_zero, by = 1)\n",
    "        k_range = k_range1[:max_eps]\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        w_all,_ = epsilon_const_path(A_u,yTrain.values, lambda_ridge, non_zero, ucounts, k_range,\n",
    "                                tol , debias =  debias, verbose = False, time_limit = time_limit, time_limit_gurobi = time_limit_gurobi)\n",
    "\n",
    "\n",
    "        mobj_temp = []\n",
    "        mobj_temp_rules = []\n",
    "        for w in w_all:\n",
    "            mobj_temp.append(r_squared(yTest,A_test_u@w,yTrain))\n",
    "            mobj_temp_rules.append(list(np.array(unique_rules)[w!=0]))\n",
    "\n",
    "        multiobj_perfs.append(mobj_temp)\n",
    "        multiobj_rules.append(mobj_temp_rules)\n",
    "\n",
    "\n",
    "\n",
    "        #RuleFit\n",
    "        mod = sklearn.linear_model.lasso_path(A_u,yTrain,n_alphas = 1000)\n",
    "        coefs = mod[1].T\n",
    "        if len(np.where([sum(c!=0)==non_zero for c in coefs])[0])>0:\n",
    "            w_l = coefs[np.where([sum(c!=0)==non_zero for c in coefs])[0][0]]\n",
    "            rulefit_rules.append(list(np.array(unique_rules)[w_l!=0]))\n",
    "            rulefit_perfs.append(r_squared(yTest,A_test_u@w_l,yTrain))\n",
    "        #####\n",
    "        print('fold',i, non_zero)\n",
    "    \n",
    "    \n",
    "    res_dict = {}\n",
    "    res_dict['sirus_rules'] =  sirus_rules\n",
    "    res_dict['sirus_perfs'] =  sirus_perfs\n",
    "    res_dict['rulefit_rules'] =  rulefit_rules\n",
    "    res_dict['rulefit_perfs'] =  rulefit_perfs\n",
    "    res_dict['multiobj_rules'] =  multiobj_rules\n",
    "    res_dict['multiobj_perfs'] =  multiobj_perfs\n",
    "    name1 = name +'_' + str(id1) +'_trees_' + str(ntrees) + '_ridge_' + \\\n",
    "            str(lambda_ridge) + '_tol_' + str(tol) +'_debias_' + str(debias) +'.pickle'\n",
    "    \n",
    "    name1 =  name1\n",
    "    \n",
    "    with open(name1, 'wb') as handle:\n",
    "        pickle.dump(res_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    multiobj_perfs = np.array(multiobj_perfs)\n",
    "    multiobj_perf_means = multiobj_perfs.mean(axis = 0)\n",
    "\n",
    "    multiobj_stability = []\n",
    "    for i in range(0,len(multiobj_rules[0])):\n",
    "        temp = []\n",
    "        for i1 in range(0,len(multiobj_rules)):\n",
    "            for i2 in range(i1+1,len(multiobj_rules)):\n",
    "                rules1 = list(get_uniques(multiobj_rules[i1][i])[0])   \n",
    "                rules2 = list(get_uniques(multiobj_rules[i2][i])[0])\n",
    "                temp.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "        multiobj_stability.append(temp)\n",
    "    multiobj_stability = np.array(multiobj_stability)\n",
    "    multiobj_stability_means = multiobj_stability.mean(axis = 1)\n",
    "    rulefit_stability= []\n",
    "    for i1 in range(0,len(rulefit_rules)):\n",
    "        for i2 in range(i1+1,len(rulefit_rules)):\n",
    "            rules1 = list(get_uniques(rulefit_rules[i1])[0])   \n",
    "            rules2 = list(get_uniques(rulefit_rules[i2])[0])\n",
    "            rulefit_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "    sirus_stability = []\n",
    "    for i1 in range(0,len(sirus_rules)):\n",
    "        for i2 in range(i1+1,len(sirus_rules)):\n",
    "            rules1 = list(get_uniques(sirus_rules[i1])[0])   \n",
    "            rules2 = list(get_uniques(sirus_rules[i2])[0])\n",
    "            sirus_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "\n",
    "    plt.scatter(1-multiobj_perf_means[1:],multiobj_stability_means[1:],label = 'multiobj', alpha = .3)\n",
    "    plt.scatter(1-multiobj_perf_means[0],multiobj_stability_means[0],label = 'multiobj_stability', color = 'C0',alpha = .4)\n",
    "\n",
    "    temp_list = list(range(1,len(multiobj_perf_means[1:]) + 1))\n",
    "    for i in range(len(multiobj_perf_means[1:],)):\n",
    "        plt.annotate(temp_list[i], (1-multiobj_perf_means[1:][i], multiobj_stability_means[1:][i]))\n",
    "\n",
    "    plt.scatter(1- np.mean(rulefit_perfs),np.mean(rulefit_stability),label = 'rulefit', color = 'green')\n",
    "    plt.scatter(1-np.mean(sirus_perfs),np.mean(sirus_stability),label = 'sirus', color = 'red')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('1 - R^2')\n",
    "    plt.ylabel('Stability')\n",
    "    name2 = name +'_' + str(id1) +'_trees_' + str(ntrees) + '_ridge_' + \\\n",
    "            str(lambda_ridge) + '_tol_' + str(tol) +'_debias_' + str(debias) +'.pdf'\n",
    "    plt.title(name2)\n",
    "    plt.savefig(name2)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5bea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
