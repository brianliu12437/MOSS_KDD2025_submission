{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from gurobipy import quicksum\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn\n",
    "from numba import jit\n",
    "import itertools\n",
    "import scipy\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import random    \n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from scipy.sparse import csc_matrix\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "def load_openml(data_id,ordinal = True, y_label = ''):\n",
    "    \"\"\"Load dataset by id from OpenML. If ordinal == True, encode categorical columns \n",
    "    via ordinal encoding. If ordinal == False then encode categorical columns with dummy vars.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset1 = sklearn.datasets.fetch_openml(data_id = data_id,as_frame = True)\n",
    "    name = dataset1.details['name']\n",
    "    X, y = dataset1.data, dataset1.target \n",
    "    data = pd.DataFrame(X,columns = dataset1.feature_names)\n",
    "\n",
    "    if len(y_label) == 0:\n",
    "        data['y'] = y\n",
    "    else:\n",
    "        data['y'] = y[y_label]\n",
    "\n",
    "    #shuffle index\n",
    "    data = data.sample(frac = 1)\n",
    "    y = data['y']\n",
    "    y = y.astype(float)\n",
    "    X = data.drop('y',axis = 1)\n",
    "\n",
    "    #encode categorical columns\n",
    "    cat = list(set(X.columns) - set(X.select_dtypes(include=np.number).columns.tolist()))\n",
    "    \n",
    "    if ordinal == True:\n",
    "        for col in cat:\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "            X[col] = X[col].fillna(max(X[col]+1))\n",
    "        \n",
    "    elif ordinal == False:\n",
    "        X = pd.get_dummies(X,columns = cat)\n",
    "    \n",
    "\n",
    "    return X,y,name\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "get np2darray representation of each rule, 1st column = (features ), 2nd column (split values)\n",
    "all representations are stacked into a list\n",
    "\"\"\"\n",
    "def get_rule_list(tree_list):\n",
    "    rule_list = []\n",
    "    for tree1 in tree_list:\n",
    "        n_nodes = tree1.tree_.node_count\n",
    "        children_left = tree1.tree_.children_left\n",
    "        children_right = tree1.tree_.children_right\n",
    "        feature = tree1.tree_.feature\n",
    "        threshold = tree1.tree_.threshold\n",
    "        value = tree1.tree_.value\n",
    "        #can also represent via values so adjacent leaf nodes are not the same...\n",
    "        \n",
    "        \n",
    "        branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "        for b in branches:\n",
    "  \n",
    "            rule = np.column_stack((feature[b[:-1]],threshold[b[:-1]]))\n",
    "            rule_set = set()\n",
    "            i1 = 0\n",
    "            for r in rule:\n",
    "                if b[i1+1] == children_left[b[i1]]:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'L')\n",
    "                else:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'R')\n",
    "                i1 = i1 + 1\n",
    "                \n",
    "            rule_list.append(rule_set)\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "    \n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "    \n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "\n",
    "def get_uniques(rule_list):\n",
    "    unique_rules = [rule_list[0]]\n",
    "    inds = [0]\n",
    "    ind = 1\n",
    "    for r in rule_list[1:]:\n",
    "        \n",
    "        counter = 0\n",
    "        for r1 in unique_rules:\n",
    "            counter = counter + int(r != r1)\n",
    "        if counter == len(unique_rules):\n",
    "            unique_rules.append(r)\n",
    "            inds.append(ind)\n",
    "            \n",
    "        ind = ind + 1\n",
    "            \n",
    "    counts = []\n",
    "    for u in unique_rules:\n",
    "        counter = 0\n",
    "        for j in rule_list:\n",
    "            if u == j:\n",
    "                counter = counter + 1\n",
    "        counts.append(counter)\n",
    "    return np.array(unique_rules), np.array(inds),np.array(counts)\n",
    "\n",
    "\n",
    "def get_tree_matrix_sparse(X,tree1):\n",
    "    leaf_all = np.where(tree1.tree_.feature < 0)[0]\n",
    "    leaves_index = tree1.apply(X.values)\n",
    "    leaves = np.unique(leaves_index)\n",
    "    values = np.ndarray.flatten(tree1.tree_.value)\n",
    "    leaves_values = [values[i] for i in leaves_index]\n",
    "    df = pd.DataFrame(np.column_stack((range(0,len(leaves_index)),leaves_index,leaves_values))\n",
    "             ,columns = ['instance','node','value'])\n",
    "    setdiff = list(set(leaf_all) - set(np.unique(leaves_index)))\n",
    "    toadd = pd.DataFrame(np.column_stack((np.zeros(len(setdiff)),setdiff,np.zeros(len(setdiff)))),\n",
    "                        columns = ['instance','node','value'])\n",
    "    df = df.append(toadd)\n",
    "    matrix_temp = pd.pivot_table(df, index = 'instance',columns = 'node',values = 'value').fillna(0)\n",
    "    return csc_matrix(matrix_temp.values), matrix_temp.columns.values\n",
    "\n",
    "def get_rule_matrix_sparse(X,tree_list):\n",
    "    matrix_full, nodes = get_tree_matrix_sparse(X,tree_list[0])\n",
    "    node_list = [nodes]\n",
    "    for tree1 in tree_list[1:]:\n",
    "        matrix_temp, nodes = get_tree_matrix_sparse(X,tree1)\n",
    "        node_list.append(nodes)\n",
    "        matrix_full = scipy.sparse.hstack([matrix_full,matrix_temp])\n",
    "    return matrix_full.tocsc(), node_list # node list gives the indicies of the nodes in the tree structure\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "\n",
    "def quantize_X(xTrain,xTest,nbins = 10):\n",
    "    cols = xTrain.columns\n",
    "    to_replace = cols[[(len(np.unique(xTrain[c]))>=nbins) for c in cols]]\n",
    "    #pd.qcut(xTrain.rank(method='first')[col, bins)\n",
    "    for col in to_replace:\n",
    "        xTrain[col],bins = pd.qcut(xTrain[col],nbins,labels=False,retbins=True,duplicates = 'drop')\n",
    "        bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "        xTest[col]= pd.cut(xTest[col],bins,labels=False)\n",
    "        \n",
    "    return xTrain, xTest\n",
    "\"\"\"\n",
    "get np2darray representation of each rule, 1st column = (features ), 2nd column (split values)\n",
    "all representations are stacked into a list\n",
    "\"\"\"\n",
    "def get_rule_list(tree_list):\n",
    "    rule_list = []\n",
    "    for tree1 in tree_list:\n",
    "        n_nodes = tree1.tree_.node_count\n",
    "        children_left = tree1.tree_.children_left\n",
    "        children_right = tree1.tree_.children_right\n",
    "        feature = tree1.tree_.feature\n",
    "        threshold = tree1.tree_.threshold\n",
    "        value = tree1.tree_.value\n",
    "        #can also represent via values so adjacent leaf nodes are not the same...\n",
    "        \n",
    "        \n",
    "        branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "        for b in branches:\n",
    "  \n",
    "            rule = np.column_stack((feature[b[:-1]],threshold[b[:-1]]))\n",
    "            rule_set = set()\n",
    "            i1 = 0\n",
    "            for r in rule:\n",
    "                if b[i1+1] == children_left[b[i1]]:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'L')\n",
    "                else:\n",
    "                    rule_set.add(str(r[0]) +str(r[1])+'R')\n",
    "                i1 = i1 + 1\n",
    "                \n",
    "            rule_list.append(rule_set)\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "def r_squared(yTest,pred,yTrain):\n",
    "    top = np.sum((yTest - pred)**2)\n",
    "    bottom = np.sum((yTest - np.mean(yTrain))**2)\n",
    "    return 1 - top/bottom\n",
    "\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "    \n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "    \n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "\n",
    "def get_uniques(rule_list):\n",
    "    unique_rules = [rule_list[0]]\n",
    "    inds = [0]\n",
    "    ind = 1\n",
    "    for r in rule_list[1:]:\n",
    "        \n",
    "        counter = 0\n",
    "        for r1 in unique_rules:\n",
    "            counter = counter + int(r != r1)\n",
    "        if counter == len(unique_rules):\n",
    "            unique_rules.append(r)\n",
    "            inds.append(ind)\n",
    "            \n",
    "        ind = ind + 1\n",
    "            \n",
    "    counts = []\n",
    "    for u in unique_rules:\n",
    "        counter = 0\n",
    "        for j in rule_list:\n",
    "            if u == j:\n",
    "                counter = counter + 1\n",
    "        counts.append(counter)\n",
    "    return np.array(unique_rules), np.array(inds),np.array(counts)\n",
    "\n",
    "\n",
    "def get_tree_matrix_sparse(X,tree1):\n",
    "    leaf_all = np.where(tree1.tree_.feature < 0)[0]\n",
    "    leaves_index = tree1.apply(X.values)\n",
    "    leaves = np.unique(leaves_index)\n",
    "    values = np.ndarray.flatten(tree1.tree_.value)\n",
    "    leaves_values = [values[i] for i in leaves_index]\n",
    "    df = pd.DataFrame(np.column_stack((range(0,len(leaves_index)),leaves_index,leaves_values))\n",
    "             ,columns = ['instance','node','value'])\n",
    "    setdiff = list(set(leaf_all) - set(np.unique(leaves_index)))\n",
    "    toadd = pd.DataFrame(np.column_stack((np.zeros(len(setdiff)),setdiff,np.zeros(len(setdiff)))),\n",
    "                        columns = ['instance','node','value'])\n",
    "    df = df.append(toadd)\n",
    "    matrix_temp = pd.pivot_table(df, index = 'instance',columns = 'node',values = 'value').fillna(0)\n",
    "    return csc_matrix(matrix_temp.values), matrix_temp.columns.values\n",
    "\n",
    "def get_rule_matrix_sparse(X,tree_list):\n",
    "    matrix_full, nodes = get_tree_matrix_sparse(X,tree_list[0])\n",
    "    node_list = [nodes]\n",
    "    for tree1 in tree_list[1:]:\n",
    "        matrix_temp, nodes = get_tree_matrix_sparse(X,tree1)\n",
    "        node_list.append(nodes)\n",
    "        matrix_full = scipy.sparse.hstack([matrix_full,matrix_temp])\n",
    "    return matrix_full.tocsc(), node_list # node list gives the indicies of the nodes in the tree structure\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from numba import jit\n",
    "from gurobipy import quicksum\n",
    "### Entire path functions\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_regression_cost_subgradient(X,y,s,lambda_ridge):\n",
    "    \"\"\"\n",
    "    Must all be numpy arrays\n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    X_sub = X[:,s==1]\n",
    "    \n",
    "    alpha_star = y - X_sub@np.linalg.inv(np.diag(np.ones(np.sum(s))/lambda_ridge) + \\\n",
    "                                  np.transpose(X_sub)@X_sub)@np.transpose(X_sub)@y\n",
    "    \n",
    "    c = 0.5*y@alpha_star\n",
    "    \n",
    "    subgradients = np.zeros(p)\n",
    "    for j in range(0,p):\n",
    "        subgradients[j] = -lambda_ridge*0.5 *(X[:,j]@alpha_star)**2\n",
    "    \n",
    "    return c, alpha_star , subgradients\n",
    "\n",
    "\n",
    "def get_stability_path_k(ucounts, nonzero, by = 1):\n",
    "    start = 0\n",
    "    end = nonzero\n",
    "    sorted1 = np.sort(ucounts)\n",
    "\n",
    "    K_range = []\n",
    "    while start < len(sorted1):\n",
    "        K_range.append(np.sum(sorted1[start:end]))\n",
    "        start = start + 1\n",
    "        end = end + 1\n",
    "    K_range = np.flip(np.unique(K_range))\n",
    "    return K_range[::by]\n",
    "\n",
    "def epsilon_const_path(X,y, lambda_ridge, nonzero, ucounts, k_range,\n",
    "                            tol,debias =  False, verbose = False, time_limit = 120, time_limit_gurobi = 60):\n",
    "    regressor = gp.Model()\n",
    "    dim = X.shape[1]\n",
    "    nu = regressor.addVar(name=\"nu\") # Weights\n",
    "    s_new = regressor.addVars(dim, vtype=GRB.BINARY, name=\"s_new\") \n",
    "\n",
    "    regressor.setObjective(nu, GRB.MINIMIZE)\n",
    "    regressor.addConstr(quicksum(s_new) <= nonzero) # Budget constraint\n",
    "    regressor.addConstr(nu>=0)\n",
    "\n",
    "\n",
    "    #### settings\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = time_limit_gurobi\n",
    "    regressor.params.mipgap = 0.001    \n",
    "\n",
    "\n",
    "    #stability_budget = regressor.addVar(name=\"stability_budget\")\n",
    "    #regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= stability_budget)\n",
    "\n",
    "    stab_constraint = regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= 0)\n",
    "    results_w = []\n",
    "    results_obj = []\n",
    "    for k in k_range:\n",
    "\n",
    "        regressor.remove(stab_constraint)\n",
    "        stab_constraint = regressor.addConstr( quicksum((s_new[j])*ucounts[j] for j in range(dim)) >= k)\n",
    "\n",
    "        #regressor.setAttr(\"LB\", stability_budget, k)\n",
    "        #regressor.setAttr(\"UB\", stability_budget, k)\n",
    "\n",
    "        cost = 10**9\n",
    "        nu1 = 0\n",
    "\n",
    "\n",
    "        t1 = time.time()\n",
    "        while  (nu1 - cost)/cost < -tol:\n",
    "            regressor.optimize()\n",
    "            s = np.array([s_new[i].X for i in range(dim)])\n",
    "            nu1 = nu.X\n",
    "            s = s.astype(int)\n",
    "            cost, alpha_star, subgradient = get_regression_cost_subgradient(X,y,s,lambda_ridge)  \n",
    "            regressor.addConstr(nu >= cost + quicksum([ subgradient[i]*(s_new[i] -s[i]) for i in range(dim)]))\n",
    "            if (time.time() - t1) > time_limit:\n",
    "                break\n",
    "            \n",
    "            #print(len(regressor.getConstrs()))\n",
    "\n",
    "        results_obj.append([nu1,cost])\n",
    "        X_sub = X[:,s==1]\n",
    "        if debias == True:\n",
    "            inv = np.linalg.inv(np.transpose(X_sub)@X_sub + np.diag(np.ones(np.sum(s))*0.001))\n",
    "        else:\n",
    "            inv = np.linalg.inv(np.transpose(X_sub)@X_sub + np.diag(np.ones(np.sum(s))/lambda_ridge))\n",
    "\n",
    "        w_sub = inv@np.transpose(X_sub)@y\n",
    "        w = np.zeros(len(s))\n",
    "        w[s==1] = w_sub\n",
    "        results_w.append(w)\n",
    "        print(k,nu1,cost, time.time()-t1)\n",
    "        \n",
    "    return np.array(results_w),results_obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "bins = 10\n",
    "ntrees = 500\n",
    "non_zero1 = 15\n",
    "verbose = False\n",
    "time_limit = 600\n",
    "time_limit_gurobi = 120\n",
    "debias = True\n",
    "tol = 0.05\n",
    "\n",
    "\n",
    "\n",
    "ids = [\n",
    "195,\n",
    "505,\n",
    "560,\n",
    "690,\n",
    "519,\n",
    "196,\n",
    "1027,\n",
    "547,\n",
    "223,\n",
    "541,\n",
    "41021,\n",
    "315,\n",
    "507,\n",
    "529,\n",
    "183,\n",
    "42570,\n",
    "405,\n",
    "294,\n",
    "287,\n",
    "503,\n",
    "308,\n",
    "558,\n",
    "227,\n",
    "189,\n",
    "296,\n",
    "201,\n",
    "216,\n",
    "537,\n",
    "574,\n",
    "344,\n",
    "215,\n",
    "564,\n",
    "42225]\n",
    "print(len(ids))\n",
    "\n",
    "import aix360\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "from aix360.algorithms.rbm import LinearRuleRegression\n",
    "fb = FeatureBinarizer(negations=True, returnOrd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "for id1 in ids:\n",
    "    try:\n",
    "        X1,y1,name = load_openml(id1,ordinal = False, y_label = '')\n",
    "        lambda_ridge = 0.01\n",
    "        if X1.shape[0] <= 1000:\n",
    "            lambda_ridge = 0.05\n",
    "\n",
    "        print(name,X1.shape,lambda_ridge)\n",
    "        glrm_rules = []\n",
    "        glrm_perfs = []\n",
    "        sirus_rules = []\n",
    "        sirus_perfs = []\n",
    "        rulefit_rules = []\n",
    "        rulefit_perfs = []\n",
    "\n",
    "        multiobj_rules = []\n",
    "        multiobj_perfs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        kf = KFold(n_splits=10)\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X1)):\n",
    "            xTrain = X1.iloc[train_index]\n",
    "            yTrain = y1.iloc[train_index]\n",
    "            xTest = X1.iloc[test_index]\n",
    "            yTest = y1.iloc[test_index]\n",
    "            xTrain = xTrain.fillna(xTrain.median())\n",
    "            xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "\n",
    "\n",
    "            xTrain,xTest = quantize_X(xTrain,xTest,nbins = bins)\n",
    "            yscaler = preprocessing.StandardScaler()\n",
    "            yTrain = yscaler.fit_transform(yTrain.values.reshape(-1, 1))\n",
    "            yTest = yscaler.transform(yTest.values.reshape(-1, 1))\n",
    "            yTrain = pd.Series(yTrain.flatten())\n",
    "            yTrain.index = xTrain.index\n",
    "            yTest = pd.Series(yTest.flatten())\n",
    "            yTest.index = xTest.index\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            rf = RandomForestRegressor(n_estimators = ntrees, n_jobs = -1,\n",
    "                                   max_features = .3333,max_depth = 2).fit(xTrain,yTrain)\n",
    "\n",
    "            A, _ = get_rule_matrix_sparse(xTrain,rf.estimators_)\n",
    "            A_test, _ = get_rule_matrix_sparse(xTest,rf.estimators_)\n",
    "            A = A.toarray()\n",
    "            A_test = A_test.toarray()\n",
    "\n",
    "\n",
    "\n",
    "            tree_list = rf.estimators_\n",
    "            rule_list = get_rule_list(tree_list)\n",
    "            unique_rules, uindex,  ucounts = get_uniques(rule_list)\n",
    "            p_unique = len(unique_rules)\n",
    "\n",
    "            inds_top = np.argsort(-ucounts)[:non_zero1]\n",
    "\n",
    "            if (np.sort(ucounts)[::-1][non_zero1-1] == np.sort(ucounts)[::-1][non_zero1]):\n",
    "                inds_top = inds_top[ucounts[inds_top] != np.sort(ucounts)[::-1][non_zero1]]\n",
    "\n",
    "            non_zero = len(inds_top)\n",
    "            sirus = list(np.array(unique_rules)[inds_top])\n",
    "            sirus_rules.append(sirus)\n",
    "\n",
    "            A_u = A[:,uindex]\n",
    "            A_test_u = A_test[:,uindex]\n",
    "\n",
    "            A1 = A_u[:,inds_top]\n",
    "            A_test1 = A_test_u[:,inds_top]\n",
    "            ridge = 0.001\n",
    "            alpha = np.dot((np.dot(np.linalg.inv(np.dot(A1.T,A1) + np.diag(np.repeat(ridge,len(A1[0])))),A1.T)),yTrain)\n",
    "            sirus_perfs.append(r_squared(yTest,A_test1@alpha,yTrain))\n",
    "            dfTrain, dfTrainStd = fb.fit_transform(xTrain)\n",
    "            dfTest, dfTestStd = fb.transform(xTest)\n",
    "            GLRM_temp = []\n",
    "            for lambda0 in np.flip(np.logspace(-5,0,200)):\n",
    "                lrr = LinearRuleRegression(lambda0=lambda0, lambda1=0.005\n",
    "                                           , useOrd=True, debias = True)\n",
    "                lrr.fit(dfTrain, yTrain, dfTrainStd)\n",
    "                nrules = len(lrr.explain(highDegOnly = True))-1\n",
    "                GLRM_temp.append([lambda0,nrules])\n",
    "                if nrules >= non_zero-1:\n",
    "                    break\n",
    "\n",
    "            print('NZ,GLRM # Rules:', non_zero,nrules)\n",
    "            rules_glrm = lrr.explain(highDegOnly = True)\n",
    "            rules_glrm = rules_glrm['rule'].values[1:]\n",
    "            rules_glrm = [set(k.replace(\" \", \"\").split('AND')) for k in rules_glrm]\n",
    "            GLRM_score = r_squared(yTest, lrr.predict(dfTest, dfTestStd),yTrain)\n",
    "            glrm_rules.append(rules_glrm)\n",
    "            glrm_perfs.append(GLRM_score)\n",
    "\n",
    "        sirus_stability = []\n",
    "        for i1 in range(0,len(sirus_rules)):\n",
    "            for i2 in range(i1+1,len(sirus_rules)):\n",
    "                rules1 = list(get_uniques(sirus_rules[i1])[0])   \n",
    "                rules2 = list(get_uniques(sirus_rules[i2])[0])\n",
    "                sirus_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "        glrm_stability = []\n",
    "        for i1 in range(0,len(glrm_rules)):\n",
    "            for i2 in range(i1+1,len(glrm_rules)):\n",
    "                rules1 = list(get_uniques(glrm_rules[i1])[0])   \n",
    "                rules2 = list(get_uniques(glrm_rules[i2])[0])\n",
    "                glrm_stability.append(2*len(intersection(rules1,rules2))/(len(rules1)+len(rules2)))\n",
    "\n",
    "        print('sirus',np.mean(sirus_stability),np.mean(sirus_perfs))\n",
    "        print('glrm', np.mean(glrm_stability), np.mean(glrm_perfs))    \n",
    "\n",
    "        res_dict = {}\n",
    "        res_dict['sirus_rules'] =  sirus_rules\n",
    "        res_dict['sirus_perfs'] =  sirus_perfs\n",
    "        res_dict['glrm_perfs'] =  glrm_perfs\n",
    "        res_dict['glrm_rules'] =  glrm_rules\n",
    "\n",
    "        name1 = name +'_' + str(id1)\n",
    "        name1 = 'glrm_results/' + name1 +'.pickle'\n",
    "\n",
    "        with open(name1, 'wb') as handle:\n",
    "            pickle.dump(res_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    except: #the GLRM implementation occaisonally fails\n",
    "        print('failed:',name)\n",
    "        failed.append(id1)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
